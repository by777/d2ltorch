{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.0152,  0.0988,  0.0838,  0.0666, -0.3508, -0.0917, -0.0414,  0.1041,\n         -0.1836,  0.0698],\n        [ 0.0444,  0.1062,  0.0460,  0.0286, -0.3669, -0.0254,  0.0590,  0.0841,\n         -0.1432,  0.0828]], grad_fn=<AddmmBackward>)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(20, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1.1 自定义块\n",
    "要想直观地了解块是如何工作的，最简单的方法就是自己实现一个。 在实现我们自定义块之前，我们简要总结一下每个块必须提供的基本功能：\n",
    "+ 将输入数据作为其前向传播的参数\n",
    "+ 通过前向传播函数来生成输出。\n",
    "+ 计算其输出关于输入的梯度\n",
    "+ 存储和访问前向传播计算所需要的参数\n",
    "+ 根据需要初始化模型参数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "    def forward(self, x):\n",
    "        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.0431,  0.4604,  0.0293,  0.2095, -0.0230, -0.0433,  0.0768, -0.0257,\n         -0.0326,  0.2002],\n        [-0.0325,  0.4500,  0.0673,  0.2576, -0.0843, -0.0553,  0.0459, -0.1746,\n         -0.0044,  0.1473]], grad_fn=<AddmmBackward>)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1.2 顺序块\n",
    "现在我们可以更仔细地看看Sequential类是如何工作的， 回想一下Sequential的设计是为了把其他模块串起来。 为了构建我们自己的简化的MySequential， 我们只需要定义两个关键函数：\n",
    "1. 一种将块诸葛追加到列表中的函数\n",
    "2. 一种前向传播函数，用于将输入按追加快的顺序传递给块组成的“链条”\n",
    "\n",
    "下面的MySequential类提供了与默认Sequential类相同的功能。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n",
    "            # 变量_modules中。module的类型是OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.0715,  0.0201, -0.1685, -0.0787,  0.0659, -0.0140, -0.2452, -0.0751,\n         -0.0005,  0.1615],\n        [ 0.0874,  0.2286, -0.1283, -0.0420,  0.0985, -0.0346, -0.2000, -0.0069,\n         -0.0183,  0.0970]], grad_fn=<AddmmBackward>)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1.3 在前向传播函数中执行代码\n",
    "Sequential类使模型构造变得简单， 允许我们组合新的架构，而不必定义自己的类。 然而，并不是所有的架构都是简单的顺序架构。 当需要更强的灵活性时，我们需要定义自己的块。 例如，我们可能希望在前向传播函数中执行Python的控制流。 此外，我们可能希望执行任意的数学运算， 而不是简单地依赖预定义的神经网络层。\n",
    "\n",
    "到目前为止，\n",
    "我们网络中的所有操作都对网络的激活值及网络的参数起作用。\n",
    "然而，有时我们可能希望合并既不是上一层的结果也不是可更新参数的项，\n",
    "我们称之为*常数参数*（constant parameter）。\n",
    "例如，我们需要一个计算函数\n",
    "$f(\\mathbf{x},\\mathbf{w}) = c \\cdot \\mathbf{w}^\\top \\mathbf{x}$的层，\n",
    "其中$\\mathbf{x}$是输入，\n",
    "$\\mathbf{w}$是参数，\n",
    "$c$是某个在优化过程中没有更新的指定常量。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FixedHiddenMLP, self).__init__()\n",
    "        # 不计算梯度的随机权重参数，因此在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20,20), requires_grad=True)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        # 使用创建的常量参数以及relu和mm函数\n",
    "        x = F.relu(torch.mm(x, self.rand_weight) + 1)\n",
    "        # 复用全连接层，相当于两个全连接层共享参数\n",
    "        x = self.linear(x)\n",
    "        # 控制流\n",
    "        while x.abs().sum() > 1:\n",
    "            x /= 2\n",
    "        return x.sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在这个FixedHiddenMLP模型中，我们实现了一个隐藏层， 其权重（self.rand_weight）在实例化时被随机初始化，之后为常量。 这个权重不是一个模型参数，因此它永远不会被反向传播更新。 然后，神经网络将这个固定层的输出通过一个全连接层。\n",
    "\n",
    "注意，在返回输出之前，模型做了一些不寻常的事情： 它运行了一个while循环，在\\(L_1\\)范数大于\\(1\\)的条件下， 将输出向量除以\\(2\\)，直到它满足条件为止。 最后，模型返回了X中所有项的和。 注意，此操作可能不会常用于在任何实际任务中， 我们只是向你展示如何将任意代码集成到神经网络计算的流程中。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.0939, grad_fn=<SumBackward0>)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(-0.3544, grad_fn=<SumBackward0>)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们可以混合搭配各种组合块的方法。 在下面的例子中，我们以一些想到的方法嵌套块。\n",
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NestMLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(20, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 32), nn.ReLU()\n",
    "        )\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "    def forward(self, x):\n",
    "        return self.linear(self.net(x))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}